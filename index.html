
<html>

<head>
  
  
  
    <base target="_blank">
    <link rel='shortcut icon' href='./logo1.jpg' />
    <meta charset="UTF-8">
    <title>Zhun Zhong's Homepage</title>
    <style>
       @import url(http://www-bcf.usc.edu/~iacopoma/style.css);
    </style>

    <!-- Bibtex -->
    <script type="text/javascript" src="http://www-bcf.usc.edu/~iacopoma/js/hidebib.js"></script>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-76826574-1', 'auto');
      ga('send', 'pageview');

    </script>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic"
     rel="stylesheet" type="text/css" />
	
	
	 <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?10dec223ee1df5e4fa433c7ffe3c909c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
	
    </head>

    <body>

    <div style="margin-bottom: 1em; border: 2px solid #eee; background-color: #fff; padding: 1em; height: 200px; border-radius: 10px;">
      <div style="margin: 0 auto; width: 70%; line-height: 130%;">
        <img title="Zhun Zhong" style="float: right; padding-right: .7em; height:
        200px;"
        src="./my_1.jpg"
  onmouseover="document.getElementById('mypic').src='./my_2.jpg';"     onmouseout="document.getElementById('mypic').src='./my_1.jpg';"
        id="mypic"
        />
        <br /><br />
        <div style="padding-right: .5em; vertical-align: top; height: 250px;">
          <span style="font-size: 20pt; line-height: 130%;"> Zhun Zhong 钟准 </span><br /><br />
          <span>PhD Student at XMU, Xiamen University</span><br /><br />
          <span><i>zhunzhong@stu.xmu.edu.cn</i></span> <br /><br />
          <a href="https://scholar.google.com/citations?user=nZizkQ0AAAAJ&hl=en">Google
          Scholar</a>/
	<a href="https://github.com/zhunzhong07">GitHub</a>
        </div>
      </div>
    </div>

    <!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->
    <div style="clear: both;">

    <div class="section">
    <h2>About me</h2>
      <div class="paper">
        Hello! <br /><br /> 
        I am currently a Ph.D. student in Cognitive Science Department, 
		      at <strong>Xiamen University</strong> under the supervision of <a href="http://information.xmu.edu.cn/portal/node/106">Prof. Shaozi Li </a>. I received the M.S. Degree in Computer Science and Technology in 2015 from <strong>China University Of Petroleum</strong>, Qingdao, China. I received the B.S. degree from the Information Engineering Department, from <strong>East China University of Technology</strong> in 2012. <br /><br /> 
 I am currently doing person re-identification research, under the co-supervision of  <a href="http://www.liangzheng.com.cn">Dr. Liang Zheng.</a> <br /><br />
 My research interests include <strong>person re-identification</strong>, <strong>object detection</strong>, and <strong>machine learning</strong>.
        
      </div>
    </div>

      
      
    <div class="section">
      <h2>News</h2>
      <div class="paper">
        <ul>
            <li><strong>[2017.03.04]</strong> One paper is accepted to CVPR 2017. <img src="http://www-bcf.usc.edu/~iacopoma/img/rss-2x.png" /></li>
            <li><strong>[2016.02.26]</strong> One paper is accepted to Neurocomputing 2017. <img src="http://www-bcf.usc.edu/~iacopoma/img/rss-2x.png" /></li>
            <li><strong>[2016.12.01]</strong> ID-discriminative Embedding baseline on Market-1501 is available. <a href="https://github.com/zhunzhong07/IDE-baseline-Market-1501">[code]</a> <img src="http://www-bcf.usc.edu/~iacopoma/img/rss-2x.png" /></li>
            <li><strong>[2016.09.01]</strong> One paper is accepted to Multimedia Tools and Applications 2016. <img src="http://www-bcf.usc.edu/~iacopoma/img/rss-2x.png" /></li>
        
        </ul>
      </div>
    </div>


    <div class="section">
    <h2 id="reports">Publications</h2>
      
       <div class="paper" id="Reranking-reid">
      <img class="paper" title="Re-ranking Person Re-identification with k-reciprocal Encoding" src="./re-reid.jpg" />
      <div>
        <a class="paper" href="https://arxiv.org/pdf/1701.08398.pdf" target="_blank">Re-ranking Person Re-identification with k-reciprocal Encoding</a><br />
        <strong>Zhun Zhong</strong>, Liang Zheng, Donglin Cao,Shaozi Li<br />
        To appear in CVPR, 2017<br />
        <a shape="rect" href="javascript:toggleabs('Reranking-reid')" class="toggleabs">abstract</a> /
        <a shape="rect" href="javascript:togglebib('Reranking-reid')" class="togglebib">bibtex</a>  /
        <a shape="rect" href="https://arxiv.org/pdf/1701.08398.pdf" target="_blank" >axXiv</a>/ 
        <a shape="rect" href="https://github.com/zhunzhong07/person-re-ranking" target="_blank" >code is available</a>/
        <pre xml:space="preserve">
@article{zhong2017re,
  title={Re-ranking Person Re-identification with k-reciprocal Encoding},
  author={Zhong, Zhun and Zheng, Liang and Cao, Donglin and Li, Shaozi},
  journal={arXiv preprint arXiv:1701.08398},
  year={2017}
}
        </pre>
        <span class="blurb">
When considering person re-identification (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy. Yet in the re-ID community, limited effort has been devoted to re-ranking, especially those fully automatic, unsupervised solutions. In this paper, we propose a k-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is that if a gallery image is similar to the probe in the k-reciprocal nearest neighbors, it is more likely to be a true match. Specifically, given an image, a k-reciprocal feature is calculated by encoding its k-reciprocal nearest neighbors into a single vector, which is used for re-ranking under the Jaccard distance. The final distance is computed as the combination of the original distance and the Jaccard distance. Our re-ranking method does not require any human interaction or any labeled data, so it is applicable to large-scale datasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW datasets confirm the effectiveness of our method.

        </span>
      </div>
      <div class="spanner"></div>
    </div>
 </div>         
      
      
      
      
       <div class="paper" id="Re-proposals">
      <img class="paper" title="Class-Specific Object Proposals Re-ranking for Object Detection in Automatic Driving" src="./re-proposals.jpg" />
      <div>
        <a class="paper" href="http://www.sciencedirect.com/science/article/pii/S0925231217303922" target="_blank">Class-Specific Object Proposals Re-ranking for Object Detection in Automatic Driving</a><br />
        <strong>Zhun Zhong</strong>, Mingyi Lei, Donglin Cao, Jianping Fan, Shaozi Li<br />
        Neurocomputing, 2017<br />
        <a shape="rect" href="javascript:toggleabs('Re-proposals')" class="toggleabs">abstract</a> /
        <a shape="rect" href="javascript:togglebib('Re-proposals')" class="togglebib">bibtex</a>  /
        <a shape="rect" href="http://www.sciencedirect.com/science/article/pii/S0925231217303922" target="_blank" >PDF</a> 
        <pre xml:space="preserve">
@article{zhong2017class,
  title={Class-Specific Object Proposals Re-ranking for Object Detection in Automatic Driving},
  author={Zhong, Zhun and Lei, Mingyi and Cao, Donglin and Fan, Jianping and Li, Shaozi},
  journal={Neurocomputing},
  year={2017},
  publisher={Elsevier}
}
        </pre>
        <span class="blurb">
        Object detection often suffers from a plenty of bootless proposals, selecting high quality proposals
remains a great challenge. In this paper, we propose a semantic, class-specific approach to re-rank
object proposals, which can consistently improve the recall performance even with less proposals.
We first extract features for each proposal including semantic segmentation, stereo information,
contextual information, CNN-based objectness and low-level cue, and then score them using classspecific
weights learnt by Structured SVM. The advantages of the proposed model are two-fold: 1)
it can be easily merged to existing generators with few computational costs, and 2) it can achieve
high recall rate uner strict critical even using less proposals. Experimental evaluation on the KITTI
benchmark demonstrates that our approach significantly improves existing popular generators on
recall performance. Moreover, in the experiment conducted for object detection, even with 1,500
proposals, our approach can still have higher average precision (AP) than baselines with 5,000
proposals.

        </span>
      </div>
      <div class="spanner"></div>
    </div>
 </div>      
      
      
      
       
        <div class="paper" id="GCP">
      <img class="paper" title="Detecting Ground Control Points via Convolutional Neural Network for Stereo Matching" src="./mtap.jpg" />
      <div>
        <a class="paper" href="http://rd.springer.com/article/10.1007/s11042-016-3932-y" target="_blank">Detecting Ground Control Points via Convolutional Neural Network for Stereo Matching</a><br />
        <strong>Zhun Zhong</strong>, Songzhi Su, Donglin Cao, Shaozi Li, Zhihan Lv<br />
        Multimedia Tools and Applications (<strong>MTA</strong>), 2016<br />
        <a shape="rect" href="javascript:toggleabs('GCP')" class="toggleabs">abstract</a> /
        <a shape="rect" href="javascript:togglebib('GCP')" class="togglebib">bibtex</a>  /
        <a shape="rect" href="http://rd.springer.com/article/10.1007/s11042-016-3932-y" target="_blank" >PDF</a> 
        <pre xml:space="preserve">
@article{zhong2016detecting,
  title={Detecting ground control points via convolutional neural network for stereo matching},
  author={Zhong, Zhun and Su, Songzhi and Cao, Donglin and Li, Shaozi and Lv, Zhihan},
  journal={Multimedia Tools and Applications},
  pages={1--16},
  year={2016},
  publisher={Springer}
}
        </pre>
        <span class="blurb">
        In this paper, we present a novel approach to detect ground control points (GCPs) for stereo matching problem. First of all, we train a convolutional neural network (CNN) on a large stereo set, and compute the matching confidence of each pixel by using the trained CNN model. Secondly, we present a ground control points selection scheme according to the maximum matching confidence of each pixel. Finally, the selected GCPs are used to refine the matching costs, then we apply the new matching costs to perform optimization with semi-global matching algorithm for improving the final disparity maps. We evaluate our approach on the KITTI 2012 stereo benchmark dataset. Our experiments show that the proposed approach significantly improves the accuracy of disparity maps.
        </span>
      </div>
      <div class="spanner"></div>
    </div>
 </div>
  
  

  
  
          <div class="paper" id="icip">
      <img class="paper" title="Unsupervised domain adaption dictionary learning for visual recognition" src="./icip2015.jpg" />
      <div>
        <a class="paper" href="https://arxiv.org/pdf/1506.01125.pdf" target="_blank">Unsupervised domain adaption dictionary learning for visual recognition</a><br />
        <strong>Zhun Zhong</strong>, Zongming Li, Runlin Li, Xiaoxia Sun<br />
        <strong>ICIP</strong>, 2015<br />
        <a shape="rect" href="javascript:toggleabs('icip')" class="toggleabs">abstract</a> /
        <a shape="rect" href="javascript:togglebib('icip')" class="togglebib">bibtex</a>  /
        <a shape="rect" href="https://arxiv.org/pdf/1506.01125.pdf" target="_blank" >axXiv</a> 
        <pre xml:space="preserve">
@article{zhong2015unsupervised,
  title={Unsupervised domain adaption dictionary learning for visual recognition},
  author={Zhong, Zhun and Li, Zongmin and Li, Runlin and Sun, Xiaoxia},
  journal={arXiv preprint arXiv:1506.01125},
  year={2015}
}
        </pre>
        <span class="blurb">
Over the last years, dictionary learning method has been extensively applied to deal with various computer vision recognition applications, and produced state-of-the-art results. However, when the data instances of a target domain have a different distribution than that of a source domain, the dictionary learning method may fail to perform well. In this paper, we address the cross-domain visual recognition problem and propose a simple but effective unsupervised domain adaption approach, where labeled data are only from source domain. In order to bring the original data in source and target domain into the same distribution, the proposed method forcing nearest coupled data between source and target domain to have identical sparse representations while jointly learning dictionaries for each domain, where the learned dictionaries can reconstruct original data in source and target domain respectively. So that sparse representations of original data can be used to perform visual recognition tasks. We demonstrate the effectiveness of our approach on standard datasets. Our method performs on par or better than competitive state-of-the-art methods.
        </span>
      </div>
      <div class="spanner"></div>
    </div>
 </div> 

         
    </div>




    <div style="clear:both;">
     <p align="right"><font size="2">    
              <a href="http://www-bcf.usc.edu/~iacopoma/"  target="_blank">I like this website!</a>. 
              </font>
            </p> 

    <br />
    </div>

    <script xml:space="preserve" language="JavaScript">
    hideallbibs();
    hideallabs();
    </script>


    <a href="http://www.easycounter.com/"><img alt="HTML Counter"
        src="http://www.easycounter.com/counter.php?xiaozhun07"
        border="0"></a> <i><font size="2" face="Arial">unique visitors
        since Dec 2016</font></i

</body>
</html>
